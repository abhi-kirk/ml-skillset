# Experiments for Chatbot-RAG

## EDA
- Distribution of chunk sizes (markdown header vs additional token based chunking)

## Embedding model selection
- Models considered: OpenAI, FlagEmbedding, BERT
- Evaluation: 
  - public benchmarks
  - retrieval performance on JPMC docs; metrics used: MRR, accuracy
  - engineering metrics: ease of use and fine-tuning, cost

## Fine-Tuning Embedding model
- Using Maytroshka embeddings with its specific loss function

## LLM model selection
- Models considered: 
  - Open source: Falcon, Llama, Mistral, Claude
  - Commercial: GPT 


## Research Method

- *Problem formulation*

- *Research literature*

- *Experiment design*

- *Model evaluation*

- *Dealing with uncertainty*

- *Real-world experimentation*

- *Debugging and interpreting results*

- *Collaboration and communication*

- *Ethical considerations*